{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5595fe",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 476 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.2 MB 1.2 MB/s eta 0:00:01     |████████████                    | 3.1 MB 1.2 MB/s eta 0:00:05\n",
      "\u001b[?25hCollecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=d4c332f5a5e6c973083b9d2050a75ad4aee10c07d0a0bad855a6acbcba399496\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.10-py2.py3-none-any.whl (242 kB)\n",
      "\u001b[K     |████████████████████████████████| 242 kB 879 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.10\n",
      "Collecting Flask\n",
      "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 769 kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting Jinja2>=3.0\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=3.6.0 in /opt/conda/lib/python3.8/site-packages (from Flask) (4.0.1)\n",
      "Collecting click>=8.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 2.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting Werkzeug>=2.2.2\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=3.6.0->Flask) (3.4.1)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Installing collected packages: MarkupSafe, Werkzeug, Jinja2, itsdangerous, click, Flask\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 1.1.1\n",
      "    Uninstalling MarkupSafe-1.1.1:\n",
      "      Successfully uninstalled MarkupSafe-1.1.1\n",
      "  Attempting uninstall: Jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Uninstalling Jinja2-2.11.3:\n",
      "      Successfully uninstalled Jinja2-2.11.3\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "Successfully installed Flask-2.2.2 Jinja2-3.1.2 MarkupSafe-2.1.1 Werkzeug-2.2.2 click-8.1.3 itsdangerous-2.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2\n",
    "!pip install openpyxl\n",
    "!pip install Flask\n",
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad559321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c36e40d-2431-4dfc-b139-2881d1a7ef19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "print(scipy.__version__)\n",
    "from scipy.spatial.distance import cosine\n",
    "1-cosine([0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1553b2c1-e3b4-4075-aaaf-d28524a88ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'fasttext' from '/opt/conda/lib/python3.8/site-packages/fasttext/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "fasttext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500db98-c5a6-4923-9606-990c6204c2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254b1e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "#from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "patterns_ru = \"[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "patterns_kk = \"[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "patterns_en = \"[А-Яа-я0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "#stopwords_ru = stopwords.words(\"russian\")\n",
    "stopwords_ru=['другои', 'еи', 'какои', 'мои', 'неи', 'сеичас', 'такои', 'этои','и', 'в', 'во', 'не', 'что', 'он','на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне','было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n",
    "stopwords_en=['i','me', 'my', 'myself', 'we', 'our', 'ours','ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any','both', 'each', 'few', 'more', 'most', 'other','some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "stopwords_kk=['ах', 'ох', 'эх', 'ай', 'эй', 'ой', 'тағы', 'тағыда', 'әрине', 'жоқ', 'сондай', 'осындай', 'осылай', 'солай', 'мұндай', 'бұндай', 'мен', 'сен', 'ол', 'біз', 'біздер', 'олар', 'сіз', 'сіздер', 'маған', 'оған', 'саған', 'біздің', 'сіздің', 'оның', 'бізге', 'сізге','оларға', 'біздерге', 'сіздерге', 'оларға', 'менімен', 'сенімен', 'онымен', 'бізбен', 'сізбен', 'олармен', 'біздермен', 'сіздермен', 'менің', 'сенің', 'біздің', 'сіздің', 'оның', 'біздердің', 'сіздердің', 'олардың', 'маған', 'саған', 'оған', 'менен', 'сенен', 'одан', 'бізден', 'сізден', 'олардан', 'біздерден', 'сіздерден', 'олардан', 'айтпақшы', 'сонымен', 'сондықтан', 'бұл', 'осы', 'сол', 'анау', 'мынау', 'сонау', 'осынау', 'ана', 'мына', 'сона', 'әні', 'міне', 'өй', 'үйт', 'бүйт', 'біреу', 'кейбіреу', 'кейбір', 'қайсыбір', 'әрбір', 'бірнеше', 'бірдеме', 'бірнеше', 'әркім', 'әрне', 'әрқайсы', 'әрқалай', 'әлдекім', 'әлдене', 'әлдеқайдан', 'әлденеше', 'әлдеқалай', 'әлдеқашан', 'алдақашан', 'еш', 'ешкім', 'ешбір', 'ештеме', 'дәнеңе', 'ешқашан', 'ешқандай', 'ешқайсы', 'емес', 'бәрі', 'барлық', 'барша', 'бар', 'күллі', 'бүкіл', 'түгел', 'өз', 'өзім', 'өзің', 'өзінің', 'өзіме', 'өзіне', 'өзімнің', 'өзі', 'өзге', 'менде', 'сенде', 'онда', 'менен', 'сенен\\tонан', 'одан', 'ау', 'па', 'ей', 'әй', 'е', 'уа', 'уау', 'уай', 'я', 'пай', 'ә', 'о', 'оһо', 'ой', 'ие', 'аһа', 'ау', 'беу', 'мәссаған', 'бәрекелді', 'әттегенай', 'жаракімалла', 'масқарай', 'астапыралла', 'япырмай', 'ойпырмай', 'кәне', 'кәнеки', 'ал', 'әйда', 'кәні', 'міне', 'әні', 'сорап', 'қош-қош', 'пфша', 'пішә', 'құрау-құрау', 'шәйт', 'шек', 'моһ', 'тәк', 'құрау', 'құр', 'кә', 'кәһ', 'күшім', 'күшім', 'мышы', 'пырс', 'әукім', 'алақай', 'паһ-паһ', 'бәрекелді', 'ура', 'әттең', 'әттеген-ай', 'қап', 'түге', 'пішту', 'шіркін', 'алатау', 'пай-пай', 'үшін', 'сайын', 'сияқты', 'туралы', 'арқылы', 'бойы', 'бойымен', 'шамалы', 'шақты', 'қаралы', 'ғұрлы', 'ғұрлым', 'шейін', 'дейін', 'қарай', 'таман', 'салым', 'тарта', 'жуық', 'таяу', 'гөрі', 'бері', 'кейін', 'соң', 'бұрын', 'бетер', 'қатар', 'бірге', 'қоса', 'арс', 'гүрс', 'дүрс', 'қорс', 'тарс', 'тырс', 'ырс', 'барқ', 'борт', 'күрт', 'кірт', 'морт', 'сарт', 'шырт', 'дүңк', 'күңк', 'қыңқ', 'мыңқ', 'маңқ', 'саңқ', 'шаңқ', 'шіңк', 'сыңқ', 'таңқ', 'тыңқ', 'ыңқ', 'болп','былп', 'жалп', 'желп', 'қолп', 'ірк', 'ырқ', 'сарт-сұрт', 'тарс-тұрс', 'арс-ұрс', 'жалт-жалт', 'жалт-жұлт', 'қалт-қалт', 'қалт-құлт', 'қаңқ-қаңқ', 'қаңқ-құңқ', 'шаңқ-шаңқ', 'шаңқ-шұңқ', 'арбаң-арбаң', 'бүгжең-бүгжең', 'арсалаң-арсалаң', 'ербелең-ербелең', 'батыр-бұтыр', 'далаң-далаң','тарбаң-тарбаң', 'қызараң-қызараң', 'қаңғыр-күңгір', 'қайқаң-құйқаң', 'митың-митың', 'салаң-сұлаң', 'ыржың-тыржың', 'бірақ', 'алайда', 'дегенмен', 'әйтпесе', 'әйткенмен', 'себебі', 'өйткені', 'сондықтан', 'үшін', 'сайын', 'сияқты', 'туралы', 'арқылы', 'бойы', 'бойымен', 'шамалы', 'шақты', 'қаралы', 'ғұрлы', 'ғұрлым', 'гөрі', 'бері', 'кейін', 'соң', 'бұрын', 'бетер', 'қатар', 'бірге', 'қоса', 'шейін', 'дейін', 'қарай', 'таман', 'салым', 'тарта', 'жуық', 'таяу', 'арнайы', 'осындай', 'ғана', 'қана', 'тек', 'әншейін']\n",
    "\n",
    "\n",
    "def lemmatize(doc,patterns,stopwords):\n",
    "    doc = re.sub(patterns, ' ', doc)\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token and token not in stopwords:\n",
    "            token = token.strip()\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            \n",
    "            tokens.append(token.lower())\n",
    "    if len(tokens) > 0:\n",
    "        return tokens\n",
    "    return None\n",
    "\n",
    "df_ru=pd.read_excel('catalog_ru.xlsx',sheet_name='1')\n",
    "df_kk=pd.read_excel('catalog_kk.xlsx',sheet_name='1')\n",
    "df_en=pd.read_excel('catalog_en.xlsx',sheet_name='1')\n",
    "\n",
    "data_ru=df_ru['NAME'].apply(lambda x: lemmatize(x,patterns_ru,stopwords_ru)).str.join(' ').tolist()\n",
    "data_kk=df_kk['NAME'].apply(lambda x: lemmatize(x,patterns_kk,stopwords_kk)).str.join(' ').tolist()\n",
    "data_en=df_en['NAME'].apply(lambda x: lemmatize(x,patterns_en,stopwords_en)).str.join(' ').tolist()\n",
    "\n",
    "v = TfidfVectorizer(input='content',\n",
    "                    encoding='utf-8', decode_error='replace', strip_accents='unicode',\n",
    "                    lowercase=True, analyzer='word', stop_words=stopwords_ru,\n",
    "                    #token_pattern=r'(?u)\\b[а-яА-Я_][а-яА-Я0-9_]+\\b',\n",
    "                    #ngram_range=(1, 2),\n",
    "                    ngram_range=(1, 1),\n",
    "                    max_features=20000,\n",
    "                    norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
    "                    max_df=0.1, min_df=5\n",
    "                    #max_df=1, min_df=1\n",
    "                   )\n",
    "\n",
    "data_ru=[x if x!=None else '' for x in data_ru]\n",
    "tfidf_vectorizer_vectors=v.fit_transform(data_ru)\n",
    "data_temp_ru=pd.DataFrame(tfidf_vectorizer_vectors.T.todense(),index=v.get_feature_names())#[1].sort_values(ascending=False).head(30)\n",
    "\n",
    "\n",
    "import pickle\n",
    "pickle.dump(data_temp_ru,open('catalog_vec_tfidf_ru.pkl','wb'))\n",
    "pickle.dump(v,open('model_tfidf_ru.pkl','wb'))\n",
    "pickle.dump(df_ru,open('catalog_ru.pkl','wb'))\n",
    "\n",
    "data_kk=[x if x!=None else '' for x in data_kk]\n",
    "tfidf_vectorizer_vectors=v.fit_transform(data_kk)\n",
    "data_temp_kk=pd.DataFrame(tfidf_vectorizer_vectors.T.todense(),index=v.get_feature_names())#[1].sort_values(ascending=False).head(30)\n",
    "\n",
    "pickle.dump(data_temp_kk,open('catalog_vec_tfidf_kk.pkl','wb'))\n",
    "pickle.dump(v,open('model_tfidf_kk.pkl','wb'))\n",
    "pickle.dump(df_kk,open('catalog_kk.pkl','wb'))\n",
    "\n",
    "data_en=[x if x!=None else '' for x in data_en]\n",
    "tfidf_vectorizer_vectors=v.fit_transform(data_en)\n",
    "data_temp_en=pd.DataFrame(tfidf_vectorizer_vectors.T.todense(),index=v.get_feature_names())#[1].sort_values(ascending=False).head(30)\n",
    "\n",
    "pickle.dump(data_temp_en,open('catalog_vec_tfidf_en.pkl','wb'))\n",
    "pickle.dump(v,open('model_tfidf_en.pkl','wb'))\n",
    "pickle.dump(df_en,open('catalog_en.pkl','wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9e847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb51457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:86\n",
      " * Running on http://172.17.0.2:86\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "/opt/conda/lib/python3.8/site-packages/scipy/spatial/distance.py:728: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "172.17.0.1 - - [03/Sep/2022 07:16:44] \"GET / HTTP/1.1\" 200 -\n",
      "172.17.0.1 - - [03/Sep/2022 07:16:47] \"POST / HTTP/1.1\" 200 -\n",
      "172.17.0.1 - - [03/Sep/2022 07:16:57] \"POST / HTTP/1.1\" 200 -\n",
      "172.17.0.1 - - [03/Sep/2022 07:17:01] \"POST / HTTP/1.1\" 200 -\n",
      "172.17.0.1 - - [03/Sep/2022 07:17:05] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import pickle\n",
    "import time\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import fasttext\n",
    "model = fasttext.load_model('lid.176.ftz')\n",
    "\n",
    "data_temp_ru=pickle.load(open('catalog_vec_tfidf_ru.pkl','rb'))\n",
    "v_ru=pickle.load(open('model_tfidf_ru.pkl','rb'))\n",
    "df_ru=pickle.load(open('catalog_ru.pkl','rb'))\n",
    "\n",
    "data_temp_kk=pickle.load(open('catalog_vec_tfidf_kk.pkl','rb'))\n",
    "v_kk=pickle.load(open('model_tfidf_kk.pkl','rb'))\n",
    "df_kk=pickle.load(open('catalog_kk.pkl','rb'))\n",
    "\n",
    "data_temp_en=pickle.load(open('catalog_vec_tfidf_en.pkl','rb'))\n",
    "v_en=pickle.load(open('model_tfidf_en.pkl','rb'))\n",
    "df_en=pickle.load(open('catalog_en.pkl','rb'))\n",
    "\n",
    "patterns_ru = \"[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "patterns_kk = \"[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "patterns_en = \"[А-Яа-я0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "#stopwords_ru = stopwords.words(\"russian\")\n",
    "stopwords_ru=['другои', 'еи', 'какои', 'мои', 'неи', 'сеичас', 'такои', 'этои','и', 'в', 'во', 'не', 'что', 'он','на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне','было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n",
    "stopwords_en=['i','me', 'my', 'myself', 'we', 'our', 'ours','ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any','both', 'each', 'few', 'more', 'most', 'other','some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "stopwords_kk=['ах', 'ох', 'эх', 'ай', 'эй', 'ой', 'тағы', 'тағыда', 'әрине', 'жоқ', 'сондай', 'осындай', 'осылай', 'солай', 'мұндай', 'бұндай', 'мен', 'сен', 'ол', 'біз', 'біздер', 'олар', 'сіз', 'сіздер', 'маған', 'оған', 'саған', 'біздің', 'сіздің', 'оның', 'бізге', 'сізге','оларға', 'біздерге', 'сіздерге', 'оларға', 'менімен', 'сенімен', 'онымен', 'бізбен', 'сізбен', 'олармен', 'біздермен', 'сіздермен', 'менің', 'сенің', 'біздің', 'сіздің', 'оның', 'біздердің', 'сіздердің', 'олардың', 'маған', 'саған', 'оған', 'менен', 'сенен', 'одан', 'бізден', 'сізден', 'олардан', 'біздерден', 'сіздерден', 'олардан', 'айтпақшы', 'сонымен', 'сондықтан', 'бұл', 'осы', 'сол', 'анау', 'мынау', 'сонау', 'осынау', 'ана', 'мына', 'сона', 'әні', 'міне', 'өй', 'үйт', 'бүйт', 'біреу', 'кейбіреу', 'кейбір', 'қайсыбір', 'әрбір', 'бірнеше', 'бірдеме', 'бірнеше', 'әркім', 'әрне', 'әрқайсы', 'әрқалай', 'әлдекім', 'әлдене', 'әлдеқайдан', 'әлденеше', 'әлдеқалай', 'әлдеқашан', 'алдақашан', 'еш', 'ешкім', 'ешбір', 'ештеме', 'дәнеңе', 'ешқашан', 'ешқандай', 'ешқайсы', 'емес', 'бәрі', 'барлық', 'барша', 'бар', 'күллі', 'бүкіл', 'түгел', 'өз', 'өзім', 'өзің', 'өзінің', 'өзіме', 'өзіне', 'өзімнің', 'өзі', 'өзге', 'менде', 'сенде', 'онда', 'менен', 'сенен\\tонан', 'одан', 'ау', 'па', 'ей', 'әй', 'е', 'уа', 'уау', 'уай', 'я', 'пай', 'ә', 'о', 'оһо', 'ой', 'ие', 'аһа', 'ау', 'беу', 'мәссаған', 'бәрекелді', 'әттегенай', 'жаракімалла', 'масқарай', 'астапыралла', 'япырмай', 'ойпырмай', 'кәне', 'кәнеки', 'ал', 'әйда', 'кәні', 'міне', 'әні', 'сорап', 'қош-қош', 'пфша', 'пішә', 'құрау-құрау', 'шәйт', 'шек', 'моһ', 'тәк', 'құрау', 'құр', 'кә', 'кәһ', 'күшім', 'күшім', 'мышы', 'пырс', 'әукім', 'алақай', 'паһ-паһ', 'бәрекелді', 'ура', 'әттең', 'әттеген-ай', 'қап', 'түге', 'пішту', 'шіркін', 'алатау', 'пай-пай', 'үшін', 'сайын', 'сияқты', 'туралы', 'арқылы', 'бойы', 'бойымен', 'шамалы', 'шақты', 'қаралы', 'ғұрлы', 'ғұрлым', 'шейін', 'дейін', 'қарай', 'таман', 'салым', 'тарта', 'жуық', 'таяу', 'гөрі', 'бері', 'кейін', 'соң', 'бұрын', 'бетер', 'қатар', 'бірге', 'қоса', 'арс', 'гүрс', 'дүрс', 'қорс', 'тарс', 'тырс', 'ырс', 'барқ', 'борт', 'күрт', 'кірт', 'морт', 'сарт', 'шырт', 'дүңк', 'күңк', 'қыңқ', 'мыңқ', 'маңқ', 'саңқ', 'шаңқ', 'шіңк', 'сыңқ', 'таңқ', 'тыңқ', 'ыңқ', 'болп','былп', 'жалп', 'желп', 'қолп', 'ірк', 'ырқ', 'сарт-сұрт', 'тарс-тұрс', 'арс-ұрс', 'жалт-жалт', 'жалт-жұлт', 'қалт-қалт', 'қалт-құлт', 'қаңқ-қаңқ', 'қаңқ-құңқ', 'шаңқ-шаңқ', 'шаңқ-шұңқ', 'арбаң-арбаң', 'бүгжең-бүгжең', 'арсалаң-арсалаң', 'ербелең-ербелең', 'батыр-бұтыр', 'далаң-далаң','тарбаң-тарбаң', 'қызараң-қызараң', 'қаңғыр-күңгір', 'қайқаң-құйқаң', 'митың-митың', 'салаң-сұлаң', 'ыржың-тыржың', 'бірақ', 'алайда', 'дегенмен', 'әйтпесе', 'әйткенмен', 'себебі', 'өйткені', 'сондықтан', 'үшін', 'сайын', 'сияқты', 'туралы', 'арқылы', 'бойы', 'бойымен', 'шамалы', 'шақты', 'қаралы', 'ғұрлы', 'ғұрлым', 'гөрі', 'бері', 'кейін', 'соң', 'бұрын', 'бетер', 'қатар', 'бірге', 'қоса', 'шейін', 'дейін', 'қарай', 'таман', 'салым', 'тарта', 'жуық', 'таяу', 'арнайы', 'осындай', 'ғана', 'қана', 'тек', 'әншейін']\n",
    "\n",
    "morph = MorphAnalyzer()    \n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def result_v1(search_text,data_temp=None,v=None,df=None,patterns=None,stopwords=None,morph=None):\n",
    "\n",
    "    \n",
    "    #from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from scipy.spatial.distance import cosine\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def lemmatize(doc,patterns,stopwords):\n",
    "        doc = re.sub(patterns, ' ', doc)\n",
    "        tokens = []\n",
    "        for token in doc.split():\n",
    "            if token and token not in stopwords:\n",
    "                token = token.strip()\n",
    "                token = morph.normal_forms(token)[0]\n",
    "\n",
    "                tokens.append(token.lower())\n",
    "        if len(tokens) > 0:\n",
    "            return tokens\n",
    "        return None\n",
    "\n",
    "\n",
    "    a=[search_text]\n",
    "    try:\n",
    "        a=[' '.join(pd.Series(a).apply(lambda x: lemmatize(x,patterns,stopwords))[0])]\n",
    "        \n",
    "    except Exception:\n",
    "        a=['null']\n",
    "        pass\n",
    "    \n",
    "    #print(a)\n",
    "    a_vec=np.array(v.transform(a).T.todense()).flatten()\n",
    "    t=[1-cosine(a_vec, np.array((data_temp[i]))) for i in range(data_temp.shape[1])]\n",
    "\n",
    "#     from joblib import Parallel, delayed\n",
    "#     def process(i):\n",
    "#         return cosine(a_vec,i)\n",
    "#     t = Parallel(n_jobs=2)(delayed(process)(i) for i in range(data_temp.shape[1]))\n",
    "\n",
    "\n",
    "    t = np.array(t)\n",
    "    \n",
    "    \n",
    "    inds = (-t).argsort()\n",
    "    \n",
    "    top=10\n",
    "    code2=df['CODE'].loc[inds[:top]].tolist()\n",
    "    name2=df['NAME'].loc[inds[:top]].tolist()\n",
    "    score2=t[inds[:top]].tolist()\n",
    "    \n",
    "    cutoff=np.where(np.array(score2)>0.0001,True,False)\n",
    "    \n",
    "    code2=[val for is_good, val in zip(cutoff, code2) if is_good]\n",
    "    name2=[val for is_good, val in zip(cutoff, name2) if is_good]\n",
    "    score2=[val for is_good, val in zip(cutoff, score2) if is_good]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #t2=pd.DataFrame([t,df['CODE'],df['NAME']]).T.rename(columns={0:'SCORE',1:'CODE',2:'NAME'}).sort_values('SCORE',ascending=False).head(10)[['CODE','NAME','SCORE']]\n",
    "    #return df['CODE'].loc[inds[:10]].tolist(),df['NAME'].loc[inds[:10]].tolist(),t[inds[:10]].tolist()\n",
    "    return code2,name2,score2\n",
    "    \n",
    "@app.route('/', methods=['POST','GET'])    \n",
    "def my_form_post():\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    \n",
    "    a='пример' \n",
    "\n",
    "    #newList=[{'name': 'red', 'code': 'a1', 'score': 1}, {'name': 'green', 'code': 'a5', 'score': 2}, {'name': 'blue', 'code': 'a11', 'score': 3}, {'name': 'blue2', 'code': 'a2', 'score': 5}]\n",
    "    \n",
    "    if request.method==\"POST\":\n",
    "        a=request.form.get('name2')\n",
    "        \n",
    "    predict_language=model.predict(a, k=20)\n",
    "    predict_language_lan,_=predict_language\n",
    "    for i in predict_language_lan:\n",
    "        if i in ['__label__en','__label__ru','__label__kk']:\n",
    "            result=i[-2:]\n",
    "            break\n",
    "    \n",
    "    if result=='ru':\n",
    "        code,name,score=result_v1(a,data_temp_ru,v_ru,df_ru,patterns_ru,stopwords_ru,morph)\n",
    "    elif result=='kk':\n",
    "        code,name,score=result_v1(a,data_temp_kk,v_kk,df_kk,patterns_kk,stopwords_kk,morph)\n",
    "    elif result=='en':\n",
    "        code,name,score=result_v1(a,data_temp_en,v_en,df_en,patterns_en,stopwords_en,morph)       \n",
    "        \n",
    "   \n",
    "    keys=['name','code','score']\n",
    "    values=[name,code,score]\n",
    "\n",
    "    newList = []\n",
    "    for i in range(len(values[0])):\n",
    "\n",
    "        tmp = {}\n",
    "        for j, key in enumerate(keys):\n",
    "            tmp[key] = values[j][i]\n",
    "\n",
    "        newList.append(tmp)\n",
    "        \n",
    "    end = time.time()\n",
    "\n",
    "    return render_template('catalog_tfidf.html',data=newList,time_calc=round(end-start,3))        \n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run(host='0.0.0.0', port=86, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e597ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96b209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 660 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pybind11>=2.2\n",
      "  Using cached pybind11-2.10.0-py3-none-any.whl (213 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from fasttext) (49.6.0.post20210108)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from fasttext) (1.20.2)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=4690183 sha256=53cfe95e19dd2a40279f1d4d09c0ecf435875d0dae66183e2e69674b1f1b2fa6\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.2 pybind11-2.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ab611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__ru', '__label__uk', '__label__bg', '__label__mk', '__label__sr', '__label__be', '__label__hy', '__label__ar', '__label__kk', '__label__tg', '__label__fr', '__label__en', '__label__ca', '__label__rue', '__label__ja', '__label__de', '__label__cv', '__label__zh', '__label__vi', '__label__ro'), array([7.88980901e-01, 1.35625854e-01, 4.57002558e-02, 1.26464833e-02,\n",
      "       9.77900345e-03, 1.54384866e-03, 1.37968140e-03, 1.14618964e-03,\n",
      "       7.20262935e-04, 3.37156089e-04, 2.56940402e-04, 2.51930120e-04,\n",
      "       2.00188981e-04, 1.86955032e-04, 1.84934368e-04, 1.53302390e-04,\n",
      "       1.30490633e-04, 7.19720847e-05, 6.28948983e-05, 5.98976840e-05]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model.predict('привет', k=20))  # top 2 matching languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63852c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 113 µs, sys: 92 µs, total: 205 µs\n",
      "Wall time: 219 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kk'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "t=model.predict('қант', k=20)\n",
    "a,_=t\n",
    "for i in a:\n",
    "    if i in ['__label__en','__label__ru','__label__kk']:\n",
    "        result=i[-2:]\n",
    "        break\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83082d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af526a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:87\n",
      " * Running on http://172.17.0.2:87\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import pickle\n",
    "import time\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/', methods=['POST','GET'])    \n",
    "def my_form_post():\n",
    "    \n",
    "\n",
    "\n",
    "    return render_template('aa2.html')  \n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run(host='0.0.0.0', port=87, debug=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce5da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import pickle\n",
    "import time\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re\n",
    "import pandas as pd\n",
    "app = Flask(__name__)\n",
    "\n",
    "data_temp=pickle.load(open('catalog_vec_tfidf_ru.pkl','rb'))\n",
    "v=pickle.load(open('model_tfidf_ru.pkl','rb'))\n",
    "df=pickle.load(open('catalog_ru.pkl','rb'))\n",
    "patterns_ru = \"[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "#stopwords_ru = stopwords.words(\"russian\")\n",
    "stopwords_ru=['другои', 'еи', 'какои', 'мои', 'неи', 'сеичас', 'такои', 'этои','и', 'в', 'во', 'не', 'что', 'он','на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне','было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n",
    "morph = MorphAnalyzer()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd840ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def lemmatize(doc,patterns,stopwords):\n",
    "        doc = re.sub(patterns, ' ', doc)\n",
    "        tokens = []\n",
    "        for token in doc.split():\n",
    "            if token and token not in stopwords:\n",
    "                token = token.strip()\n",
    "                token = morph.normal_forms(token)[0]\n",
    "\n",
    "                tokens.append(token.lower())\n",
    "        if len(tokens) > 0:\n",
    "            return tokens\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271ef841",
   "metadata": {},
   "outputs": [],
   "source": [
    "a='одежда и sdf sdf 1231 748399&*$((' \n",
    "a=[' '.join(pd.Series(a).apply(lambda x: lemmatize(x,patterns_ru,stopwords_ru))[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e09234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['одежда']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64cd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_v1(search_text,data_temp=None,v=None,df=None,patterns=None,stopwords=None,morph=None):\n",
    "\n",
    "    \n",
    "    #from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from scipy.spatial.distance import cosine\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def lemmatize(doc,patterns,stopwords):\n",
    "        doc = re.sub(patterns, ' ', doc)\n",
    "        tokens = []\n",
    "        for token in doc.split():\n",
    "            if token and token not in stopwords:\n",
    "                token = token.strip()\n",
    "                token = morph.normal_forms(token)[0]\n",
    "\n",
    "                tokens.append(token.lower())\n",
    "        if len(tokens) > 0:\n",
    "            return tokens\n",
    "        return None\n",
    "\n",
    "\n",
    "    a=[search_text]\n",
    "    print(a)\n",
    "    try:\n",
    "        a=[' '.join(pd.Series(a).apply(lambda x: lemmatize(x,patterns,stopwords))[0])]\n",
    "        \n",
    "    except Exception:\n",
    "        a=['null']\n",
    "        pass\n",
    "    \n",
    "    print(a)\n",
    "    a_vec=np.array(v.transform(a).T.todense()).flatten()\n",
    "    print(a_vec)\n",
    "    t=[1-cosine(a_vec, np.array((data_temp[i]))) for i in range(data_temp.shape[1])]\n",
    "\n",
    "#     from joblib import Parallel, delayed\n",
    "#     def process(i):\n",
    "#         return cosine(a_vec,i)\n",
    "#     t = Parallel(n_jobs=2)(delayed(process)(i) for i in range(data_temp.shape[1]))\n",
    "\n",
    "\n",
    "    t = np.array(t)\n",
    "    \n",
    "    \n",
    "    inds = (-t).argsort()\n",
    "    \n",
    "    top=10\n",
    "    code2=df['CODE'].loc[inds[:top]].tolist()\n",
    "    name2=df['NAME'].loc[inds[:top]].tolist()\n",
    "    score2=t[inds[:top]].tolist()\n",
    "    \n",
    "    cutoff=np.where(np.array(score2)>0.0001,True,False)\n",
    "    \n",
    "    code2=[val for is_good, val in zip(cutoff, code2) if is_good]\n",
    "    name2=[val for is_good, val in zip(cutoff, name2) if is_good]\n",
    "    score2=[val for is_good, val in zip(cutoff, score2) if is_good]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #t2=pd.DataFrame([t,df['CODE'],df['NAME']]).T.rename(columns={0:'SCORE',1:'CODE',2:'NAME'}).sort_values('SCORE',ascending=False).head(10)[['CODE','NAME','SCORE']]\n",
    "    #return df['CODE'].loc[inds[:10]].tolist(),df['NAME'].loc[inds[:10]].tolist(),t[inds[:10]].tolist()\n",
    "    return code2,name2,score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f539fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(decode_error='replace', max_df=0.1, max_features=20000,\n",
       "                min_df=5,\n",
       "                stop_words=['другои', 'еи', 'какои', 'мои', 'неи', 'сеичас',\n",
       "                            'такои', 'этои', 'и', 'в', 'во', 'не', 'что', 'он',\n",
       "                            'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все',\n",
       "                            'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', ...],\n",
       "                strip_accents='unicode', sublinear_tf=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d34bc002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'одежда'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a4c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbf074b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['одежда']\n",
      "['одежда']\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/spatial/distance.py:728: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "code,name,score=result_v1(a[0],data_temp,v,df,patterns_ru,stopwords_ru,morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08748aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[481850,\n",
       " 621790,\n",
       " 611790,\n",
       " 630790,\n",
       " 430310,\n",
       " 430390,\n",
       " 620990,\n",
       " 621010,\n",
       " 620930,\n",
       " 620920]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eee010c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Предметы одежды и принадлежности к одежде',\n",
       " 'Части одежды или принадлежностей к одежде, кроме включенных в товарную позицию 6212',\n",
       " 'Части одежды или принадлежности к одежде трикотажные машинного или ручного вязания',\n",
       " 'Прочие готовые изделия, включая выкройки одежды',\n",
       " 'Предметы одежды и принадлежности к одежде из натурального меха',\n",
       " 'Прочие предметы одежды, принадлежности к одежде и прочие изделия, из натурального меха',\n",
       " 'Детская одежда и принадлежности к ней из прочих текстильных материалов',\n",
       " 'Предметы одежды, изготовленные из материалов товарной позиции 5602 или 5603',\n",
       " 'Детская одежда и принадлежности к ней из синтетических нитей',\n",
       " 'Детская одежда и принадлежности к ней из хлопчатобумажной пряжи']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97f33825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7746165300027065,\n",
       " 0.6718471746858187,\n",
       " 0.6706188710848427,\n",
       " 0.6175989894384731,\n",
       " 0.6113615700088769,\n",
       " 0.5955419786800468,\n",
       " 0.4809290415662011,\n",
       " 0.4746388065999654,\n",
       " 0.4707107301453538,\n",
       " 0.46788389319810775]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ed8da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5abbf9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "a_vec=np.array(v.transform(a).T.todense()).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73195143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 458 ms, sys: 25.5 ms, total: 484 ms\n",
      "Wall time: 447 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t=[1-cosine(a_vec, np.array((data_temp[i]))) for i in range(data_temp.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee5b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403dac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7750e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e23bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import pickle\n",
    "import time\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "app = Flask(__name__)\n",
    "\n",
    "data_temp=pickle.load(open('catalog_vec_tfidf_ru.pkl','rb'))\n",
    "v=pickle.load(open('model_tfidf_ru.pkl','rb'))\n",
    "df=pickle.load(open('catalog_ru.pkl','rb'))\n",
    "patterns = \"[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "#stopwords_ru = stopwords.words(\"russian\")\n",
    "stopwords_ru=['другои', 'еи', 'какои', 'мои', 'неи', 'сеичас', 'такои', 'этои','и', 'в', 'во', 'не', 'что', 'он','на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне','было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n",
    "morph = MorphAnalyzer()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b742c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_text='одежда'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6586ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from scipy.spatial.distance import cosine\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def lemmatize(doc):\n",
    "        doc = re.sub(patterns, ' ', doc)\n",
    "        tokens = []\n",
    "        for token in doc.split():\n",
    "            if token and token not in stopwords_ru:\n",
    "                token = token.strip()\n",
    "                token = morph.normal_forms(token)[0]\n",
    "\n",
    "                tokens.append(token.lower())\n",
    "        if len(tokens) > 0:\n",
    "            return tokens\n",
    "        return None\n",
    "\n",
    "\n",
    "    a=[search_text]\n",
    "    try:\n",
    "        a=[' '.join(pd.Series(a).apply(lemmatize)[0])]\n",
    "        \n",
    "    except Exception:\n",
    "        a=['null']\n",
    "        pass\n",
    "    \n",
    "    #print(a)\n",
    "    a_vec=np.array(v.transform(a).T.todense()).flatten()\n",
    "    t=[1-cosine(a_vec, np.array((data_temp[i]))) for i in range(data_temp.shape[1])]\n",
    "\n",
    "#     from joblib import Parallel, delayed\n",
    "#     def process(i):\n",
    "#         return cosine(a_vec,i)\n",
    "#     t = Parallel(n_jobs=2)(delayed(process)(i) for i in range(data_temp.shape[1]))\n",
    "\n",
    "\n",
    "    t = np.array(t)\n",
    "    \n",
    "    \n",
    "    inds = (-t).argsort()\n",
    "    \n",
    "    top=10\n",
    "    code2=df['CODE'].loc[inds[:top]].tolist()\n",
    "    name2=df['NAME'].loc[inds[:top]].tolist()\n",
    "    score2=t[inds[:top]].tolist()\n",
    "    \n",
    "    cutoff=np.where(np.array(score2)>0.0001,True,False)\n",
    "    \n",
    "    code2=[val for is_good, val in zip(cutoff, code2) if is_good]\n",
    "    name2=[val for is_good, val in zip(cutoff, name2) if is_good]\n",
    "    score2=[val for is_good, val in zip(cutoff, score2) if is_good]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9fbc9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(v.transform(a).T.todense()).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11f2d16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[290394,\n",
       " 842410,\n",
       " 293379,\n",
       " 630630,\n",
       " 851240,\n",
       " 901510,\n",
       " 901520,\n",
       " 901530,\n",
       " 810391,\n",
       " 281211]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7708b8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Гексабромбифенилы',\n",
       " 'Огнетушители заряженные или незаряженные',\n",
       " 'Лактамы прочие',\n",
       " 'Паруса',\n",
       " 'Стеклоочистители, антиобледенители и противозапотеватели',\n",
       " 'Дальномеры',\n",
       " 'Теодолиты и тахеометры',\n",
       " 'Нивелиры',\n",
       " 'Тигли',\n",
       " 'Дихлорид карбонила (фосген)']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c983209e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d8b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fd7118e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "       0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "       1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.36250145, 0.        , 0.37239313, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.61136157, 0.59554198,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.26405715, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582effab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
